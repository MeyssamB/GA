{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOo1+FRCU0csXcGRvSQQtpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metrogit/GA/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import  Dense\n",
        "from tensorflow.keras.losses import MAE,MSE,huber\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,mean_squared_error,mean_absolute_error\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "\n",
        "#from sklearn import cross_validation                     \n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "def create_sample(filename,n):\n",
        "    df = pd.DataFrame( columns = ['x0','x1','x2','xp0','xp1','xp2','y0','y1','y2','yp0','yp1','yp2',\n",
        "                                'Mt00', 'Mt10','Mt20','Mt30','Mt40','Mt50','Mt60', 'Mt01','Mt02','Mt03','Mt04','Mt05','Mt06']) \n",
        "    for i in range(n):\n",
        "        x0=random.uniform(0.0,2.0)\n",
        "        y0=random.random()\n",
        "        x1=random.uniform(0.0,2.0)\n",
        "        y1=random.random()\n",
        "        x2=random.uniform(0.0,2.0)\n",
        "        y2=random.random()\n",
        "        xp0=random.uniform(0.0,2.0)\n",
        "        yp0=random.random()\n",
        "        xp1=random.uniform(0.0,2.0)\n",
        "        yp1=random.random()\n",
        "        xp2=random.uniform(0.0,2.0)\n",
        "        yp2=random.random()\n",
        "\n",
        "        Mt00, Mt10,Mt20,Mt30,Mt40,Mt50,Mt60,Mt01,Mt02,Mt03,Mt04,Mt05,Mt06 = calcute_momentums(x0,y0,x1,y1,x2,y2,xp0,yp0,xp1,yp1,xp2,yp2)\n",
        "\n",
        "        new_row={'x0':x0,'y0':y0,'x1':x1,'y1':y1,'x2':x2,'y2':y2,'xp0':xp0,'yp0':yp0,'xp1':xp1,'yp1':yp1,'xp2':xp2,'yp2':yp2,\n",
        "                'Mt00':Mt00, 'Mt10':Mt10,'Mt20':Mt20,'Mt30':Mt30,'Mt40':Mt40,\n",
        "                'Mt50':Mt50,'Mt60':Mt60, 'Mt01':Mt01,'Mt02':Mt02,'Mt03':Mt03,'Mt04':Mt04,'Mt05':Mt05,'Mt06':Mt06}\n",
        "        df=df.append(new_row,ignore_index=True)\n",
        "    \n",
        "    df.to_csv(os.path.join(sys.path[0],filename))\n",
        "\n",
        "\n",
        "def calcute_momentums(x0,y0,x1,y1,x2,y2,xp0,yp0,xp1,yp1,xp2,yp2):\n",
        "    X1 = x2 + x1 + x0\n",
        "    X2 = x1 * x2 + x0 * x2 + x0 * x1\n",
        "    X3 = x0 * x1 * x2\n",
        "    Y1 = y2 + y1 + y0\n",
        "    Y2 = y1 * y2 + y0 * y2 + y0 * y1\n",
        "    Y3 = y0 * y1 * y2\n",
        "    Xp1 = xp2 + xp1 + xp0\n",
        "    Xp2 = xp1 * xp2 + xp0 * xp2 + xp0 * xp1\n",
        "    Xp3 = xp0 * xp1 * xp2\n",
        "    Yp1 = yp2 + yp1 + yp0\n",
        "    Yp2 = yp1 * yp2 + yp0 * yp2 + yp0 * yp1\n",
        "    Yp3 = yp0 * yp1 * yp2\n",
        "\n",
        "    m100 = (x0 * y1 - x1 * y0 + x1 * y2 - x2 * y1 + x2 * y0 - x0 * y2)/(xp0 * yp1 - xp1 * yp0 + xp1 * yp2 - xp2 * yp1 + xp2 * yp0 - xp0 * yp2)\n",
        "    Mt00 = -0.5*(x0 * y1 - x1 * y0 + x1 * y2 - x2 * y1 + x2 * y0 - x0 * y2)-0.5*(xp0 * yp1 - xp1 * yp0 + xp1 * yp2 - xp2 * yp1 + xp2 * yp0 - xp0 * yp2)\n",
        "    mt10 = X1/3 * m100 + Xp1/3\n",
        "    mt01 = Y1/3 * m100 + Yp1/3\n",
        "    mt20=(pow(X1,2) - X2)/6*m100 + (pow(Xp1,2) - Xp2)/6\n",
        "    mt02=(pow(Y1,2) - Y2)/6*m100 + (pow(Yp1,2) - Yp2)/6\n",
        "    mt30=(pow(X1,3) - 2*X1*X2 + X3)/10*m100 + (pow(Xp1,3) - 2*Xp1*Xp2 + Xp3)/10\n",
        "    mt03=(pow(Y1,3) - 2*Y1*Y2 + Y3)/10*m100 + (pow(Yp1,3) - 2*Yp1*Yp2 + Yp3)/10\n",
        "    mt40=(pow(X1,4) - 3*X2*pow(X1,2) + pow(X2,2) + 2*X3*X1)/15*m100 +(pow(Xp1,4) - 3*Xp2*pow(Xp1,2) + pow(Xp2,2) + 2*Xp3*Xp1)/15\n",
        "    mt04=(pow(Y1,4) - 3*Y2*pow(Y1,2) + pow(Y2,2) + 2*Y3*Y1)/15*m100 +(pow(Yp1,4) - 3*Yp2*pow(Yp1,2) + pow(Yp2,2) + 2*Yp3*Yp1)/15\n",
        "    mt50=((pow(X1,5) - 4*pow(X1,3)*X2 + 3* X3*pow(X1,2) + 3*X1*pow(X2,2) - 2*X3*X2)/21* m100 +\n",
        "         (pow(Xp1,5) - 4*pow(Xp1,3)*Xp2 + 3*Xp3*pow(Xp1,2) + 3*Xp1*pow(Xp2,2) - 2*Xp3*Xp2)/21)\n",
        "    mt05=((pow(Y1,5) - 4*pow(Y1,3)*Y2 + 3* Y3*pow(Y1,2) + 3*Y1*pow(Y2,2) - 2*Y3*Y2)/21* m100 +\n",
        "         (pow(Yp1,5) - 4*pow(Yp1,3)*Yp2 + 3*Yp3*pow(Yp1,2) + 3*Yp1*pow(Yp2,2) - 2*Yp3*Yp2)/21)\n",
        "    mt60=((pow(X1,6) - 5*pow(X1,4)*X2 + 4*pow(X1,3)*X3 + 6*pow(X1,2)*pow(X2,2) - 6*X1*X2*X3 - pow(X2,3) + pow(X3,2) )/28*m100 +\n",
        "         (pow(Xp1,6) - 5*pow(Xp1,4)*Xp2 + 4*pow(Xp1,3)*Xp3 + 6*pow(Xp1,2)*pow(Xp2,2) - 6*Xp1*Xp2*Xp3 - pow(Xp2,3) + pow(Xp3,2) )/28)\n",
        "    mt06=((pow(Y1,6) - 5*pow(Y1,4)*Y2 + 4*pow(Y1,3)*Y3 + 6*pow(Y1,2)*pow(Y2,2) - 6*Y1*Y2*Y3 - pow(Y2,3) + pow(Y3,2) )/28*m100 +\n",
        "         (pow(Yp1,6) - 5*pow(Yp1,4)*Yp2 + 4*pow(Yp1,3)*Yp3 + 6*pow(Yp1,2)*pow(Yp2,2) - 6*Yp1*Yp2*Yp3 - pow(Yp2,3) + pow(Yp3,2) )/28)\n",
        "\n",
        "    M100=(m100 * Mt00)/(m100+1)\n",
        "    M200=(Mt00)/(m100+1)\n",
        "    Mt10=mt10*(Mt00)/(m100+1)\n",
        "    Mt01=mt01*(Mt00)/(m100+1)\n",
        "    Mt20=mt20*(Mt00)/(m100+1)\n",
        "    Mt02=mt02*(Mt00)/(m100+1)\n",
        "    Mt30=mt30*(Mt00)/(m100+1)\n",
        "    Mt03=mt03*(Mt00)/(m100+1)\n",
        "    Mt40=mt40*(Mt00)/(m100+1)\n",
        "    Mt04=mt04*(Mt00)/(m100+1)\n",
        "    Mt50=mt50*(Mt00)/(m100+1)\n",
        "    Mt05=mt05*(Mt00)/(m100+1)\n",
        "    Mt60=mt60*(Mt00)/(m100+1)\n",
        "    Mt06=mt06*(Mt00)/(m100+1)\n",
        "\n",
        "    return Mt00, Mt10,Mt20,Mt30,Mt40,Mt50,Mt60,Mt01,Mt02,Mt03,Mt04,Mt05,Mt06\n",
        "\n",
        "def load_data(fileName, scale=False, shuffle=False,test_size=0.2, features=['Mt00', 'Mt10','Mt20','Mt30','Mt40','Mt50','Mt60', 'Mt01','Mt02','Mt03','Mt04','Mt05','Mt06']\n",
        "                ,targets=['x0','x1','x2','xp0','xp1','xp2','y0','y1','y2','yp0','yp1','yp2']):\n",
        "        \n",
        "    df = pd.read_csv(os.path.join(sys.path[0], fileName), engine='python',sep=',',parse_dates=True)\n",
        "    result = {}\n",
        "    \n",
        "    columns=targets.copy()\n",
        "    columns.extend(features)\n",
        "    df=df[columns]\n",
        "    \n",
        "    if scale:\n",
        "        column_scaler = {}\n",
        "        # scale the data (prices) from 0 to 1\n",
        "        for column in columns:\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
        "            column_scaler[column] = scaler\n",
        "        # add the MinMaxScaler instances to the result returned\n",
        "        result[\"column_scaler\"] = column_scaler\n",
        "    \n",
        "    sequence_data = []\n",
        "    for entry, target in zip(df[features].values, df[targets].values):\n",
        "        sequence_data.append([np.array(entry), np.array(target)])\n",
        "        \n",
        "    \n",
        "    # construct the X's and y's\n",
        "    X, y = [], []\n",
        "    for seq, target in zip(df[features].values, df[targets].values):\n",
        "        X.append(seq)\n",
        "        y.append(target)\n",
        "    # convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    # split the dataset\n",
        "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
        "                                                                               test_size=test_size, shuffle=shuffle)\n",
        "    # return the result\n",
        "    return result\n",
        "\n",
        "def create_model(output_size,input_size, optimizer, loss=\"mse\"):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_dim=input_size,activation='relu'))\n",
        "    model.add(Dense(32,input_dim=32, activation='relu'))\n",
        "    #model.add(Dense(output_size*10, input_dim=input_size*output_size*10, activation='relu'))\n",
        "    model.add(Dense(output_size,input_dim=32, activation='relu'))\n",
        "    model.compile(loss=loss, metrics=['mse', 'mae'], optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "def tensorflow_net(fileName):\n",
        "    netType=\"MLP\"     \n",
        "    # test ratio size, 0.2 is 20%\n",
        "    TEST_SIZE = 0.1\n",
        "    EPOCHS = 25\n",
        "    \n",
        "    # features to use\n",
        "    FEATURES = ['Mt00', 'Mt10','Mt20','Mt30','Mt40','Mt50','Mt60', 'Mt01','Mt02','Mt03','Mt04','Mt05','Mt06']\n",
        "    TARGETS = ['x0','x1','x2','xp0','xp1','xp2','y0','y1','y2','yp0','yp1','yp2']\n",
        "    PREDICTIONS=['x0_f','x1_f','x2_f','xp0_f','xp1_f','xp2_f','y0_f','y1_f','y2_f','yp0_f','yp1_f','yp2_f']\n",
        "\n",
        "    \n",
        "    # date now\n",
        "    date_now = time.strftime(\"%Y-%m-%d\")\n",
        "    LOSS = MAE #\"huber_loss\"\n",
        "    BATCH_SIZE=1\n",
        "    lr=.00001\n",
        "    input_size=13\n",
        "    output_size=12    \n",
        "    # model name to save, making it as unique as possible based on parameters\n",
        "    model_name = f\"{date_now}-{netType}-epoch-{EPOCHS}\"\n",
        "    \n",
        "    # create these folders if they does not exist\n",
        "    \n",
        "    if not os.path.isdir(os.path.join(sys.path[0],\"resultsMLP\")):\n",
        "        os.mkdir(os.path.join(sys.path[0],\"resultsMLP\"))\n",
        "    \n",
        "\n",
        "    # load the data\n",
        "    data = load_data(fileName, test_size=TEST_SIZE, features=FEATURES,targets=TARGETS,shuffle=False)\n",
        "    \n",
        "    # construct the model\n",
        "    OPT=tf.keras.optimizers.Adam(learning_rate=lr )\n",
        "    model = create_model(output_size=output_size,input_size=input_size, loss=LOSS, optimizer=OPT )\n",
        "    print(model.summary())\n",
        "    # some tensorflow callbacks\n",
        "    checkpointer = ModelCheckpoint(os.path.join(os.path.join(sys.path[0],\"resultsMLP\"), model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
        "        \n",
        "    history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split = 0.1,\n",
        "                    callbacks=[checkpointer],\n",
        "                    verbose=1)\n",
        "    model.save(os.path.join(os.path.join(sys.path[0],\"resultsMLP\"), model_name) + \".h5\")\n",
        "    y_predict= model.predict(data[\"X_test\"])\n",
        "    mv= model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0,return_dict=True)\n",
        "    \n",
        "    mean_absolute_error = mv['mae']\n",
        "    mean_squared_error = mv['mse']\n",
        "    print(\"Mean Absolute Error:\", mean_absolute_error)\n",
        "    print(\"Mean Squared Error:\", mean_squared_error)\n",
        "\n",
        "        \n",
        "    df_featurs=pd.DataFrame(data['X_test'],columns=FEATURES)\n",
        "    df_targets=pd.DataFrame(data['y_test'],columns=TARGETS)\n",
        "    df_prediction=pd.DataFrame(y_predict,columns=PREDICTIONS)\n",
        "    df_total=pd.concat([df_featurs,df_targets,df_prediction],axis=1)\n",
        "\n",
        "    df_total.to_csv(os.path.join(sys.path[0],\"results.csv\"))\n",
        "\n",
        "def main():\n",
        "    fileName= \"6-point_14001010.csv\"\n",
        "    #fileName= \"test.csv\"\n",
        "    create_sample(fileName,200000)      #crate samples\n",
        "    #sklearn_net(fileName)               #run a model under tensorflow \n",
        "    tensorflow_net(fileName)           #run a model unde sklearn\n",
        "\n",
        "main()\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toDRuKJWaXri",
        "outputId": "96b13a62-37fb-40eb-b8e3-5cdcd7bf2743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 32)                448       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 12)                396       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,900\n",
            "Trainable params: 1,900\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/25\n",
            "161999/162000 [============================>.] - ETA: 0s - loss: 0.4530 - mse: 0.3419 - mae: 0.4530\n",
            "Epoch 00001: val_loss improved from inf to 0.43331, saving model to resultsMLP/2021-12-31-MLP-epoch-25.h5\n",
            "162000/162000 [==============================] - 291s 2ms/step - loss: 0.4530 - mse: 0.3419 - mae: 0.4530 - val_loss: 0.4333 - val_mse: 0.3094 - val_mae: 0.4333\n",
            "Epoch 2/25\n",
            "152353/162000 [===========================>..] - ETA: 16s - loss: 0.3958 - mse: 0.2490 - mae: 0.3958"
          ]
        }
      ]
    }
  ]
}